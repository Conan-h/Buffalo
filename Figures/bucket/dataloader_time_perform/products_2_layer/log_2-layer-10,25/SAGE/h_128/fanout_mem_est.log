python3 products_fanout_25_estimate.py 
/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
/home/shuangyan/.local/lib/python3.10/site-packages/pynvml/smi.py:5: FutureWarning: The pynvml.smi module is deprecated and will be removed in the next release of pynvml. Please use pynvml_utils:
(e.g. `from pynvml_utils import nvidia_smi`)
  warnings.warn(
main start at this time 1728716116.9041603
-----------------------------------------before load data 
 Nvidia-smi: 0.85455322265625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

#nodes: 2449029
#edges: 123718024
#classes: 47
success----------------------------------------
# Nodes: 2400608
# Edges: 123718024
# Train: 196571
# Val: 39255
# Test: 2164782
# Classes: 47

----------------------------------------start of run function 
 Nvidia-smi: 0.85455322265625 GB
    Memory Allocated: 0.0  GigaBytes
Max Memory Allocated: 0.0  GigaBytes

the output layer 
self.num_batch (get_in_degree_bucketing) 25
get_in_degree_bucketing dst global nid length 196571
len(bkt)  173
len(bkt)  221
len(bkt)  374
len(bkt)  456
len(bkt)  445
len(bkt)  550
len(bkt)  655
len(bkt)  512
len(bkt)  576
len(bkt)  640
len(bkt)  624
len(bkt)  583
len(bkt)  615
len(bkt)  695
len(bkt)  673
len(bkt)  614
len(bkt)  556
len(bkt)  604
len(bkt)  584
len(bkt)  572
len(bkt)  586
len(bkt)  592
len(bkt)  648
len(bkt)  645
len(bkt)  183378
total indegree bucketing result ,  196571
the number of total output nodes match :) 
local nids of zero in-degree  []
bucket partitioner: bkt_dst_nodes_list_local length  25
---||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||-----||--
self.num_batch,  25
25
25
after degree bucketing weights of each nids list [0.0008800891281012967, 0.001124275706996454, 0.0019026204272247686, 0.0023197724995039962, 0.002263813075173856, 0.0027979712165070126, 0.00333212935784017, 0.0026046568415483465, 0.0029302389467418897, 0.003255821051935433, 0.003174425525637047, 0.0029658494894974335, 0.0031286405420942053, 0.003535618173586134, 0.0034236993249258537, 0.0031235533217005562, 0.0028284945388689074, 0.003072681117764065, 0.0029709367098910825, 0.002909890065167293, 0.0029811111506783806, 0.0030116344730402754, 0.003296518815084626, 0.0032812571539036785, 0.9328843013465873]
bkt_dst_nodes_list = self.get_in_degree_bucketing() spend:  0.011112689971923828
self.gen_batches_seeds_list(bkt_dst_nodes_list_local) spend  9.179115295410156e-05
self.has_zero_indegree_seeds  False
num_output  196571
self.output_nids  196571
output nodes length match
global output equals  True
partition total batch output list spend :  3.9549946784973145
self.buckets_partition() spend  sec:  0.011219501495361328
input layer
redundancy ratio #input/#seeds/degree
7.971098265895954
7.235294117647059
5.815508021390374
5.720394736842105
6.142022471910112
5.844848484848485
5.279607415485279
6.48583984375
6.019097222222222
5.842499999999999
6.029137529137529
5.878787878787879
6.153721075672295
5.484892086330936
5.605151064883605
5.864311889250814
6.3870080406263225
5.798197203826343
6.059571016582552
5.772552447552448
5.934666016577279
6.176597051597051
5.8046833064949
5.688888888888889
0.3953612756164862
layer  0
{1: 181, 2: 3, 3: 6, 4: 3, 5: 5, 6: 2, 7: 3, 9: 3, 10: 101}
layer  1
{1: 173}

layer  0
{1: 11, 2: 248, 3: 13, 4: 12, 5: 11, 6: 9, 7: 4, 8: 4, 9: 7, 10: 256}
layer  1
{2: 221}

layer  0
{1: 12, 2: 20, 3: 408, 4: 19, 5: 22, 6: 17, 7: 13, 8: 5, 9: 7, 10: 566}
layer  1
{3: 374}

layer  0
{1: 7, 2: 17, 3: 30, 4: 499, 5: 39, 6: 33, 7: 32, 8: 17, 9: 13, 10: 939}
layer  1
{4: 456}

layer  0
{1: 9, 2: 27, 3: 28, 4: 52, 5: 491, 6: 37, 7: 29, 8: 32, 9: 19, 10: 1302}
layer  1
{5: 445}

layer  0
{1: 9, 2: 26, 3: 35, 4: 32, 5: 61, 6: 616, 7: 44, 8: 28, 9: 34, 10: 1890}
layer  1
{6: 550}

layer  0
{1: 7, 2: 28, 3: 41, 4: 51, 5: 53, 6: 55, 7: 717, 8: 47, 9: 53, 10: 2442}
layer  1
{7: 655}

layer  0
{1: 12, 2: 18, 3: 20, 4: 45, 5: 68, 6: 50, 7: 53, 8: 580, 9: 46, 10: 2785}
layer  1
{8: 512}

layer  0
{1: 9, 2: 16, 3: 28, 4: 37, 5: 43, 6: 50, 7: 54, 8: 56, 9: 628, 10: 3299}
layer  1
{9: 576}

layer  0
{1: 6, 2: 22, 3: 52, 4: 36, 5: 57, 6: 57, 7: 84, 8: 69, 9: 53, 10: 4690}
layer  1
{10: 640}

layer  0
{1: 18, 2: 23, 3: 39, 4: 46, 5: 59, 6: 70, 7: 76, 8: 89, 9: 97, 10: 5246}
layer  1
{11: 624}

layer  0
{1: 21, 2: 35, 3: 38, 4: 49, 5: 59, 6: 61, 7: 74, 8: 83, 9: 72, 10: 5148}
layer  1
{12: 583}

layer  0
{1: 12, 2: 12, 3: 35, 4: 51, 5: 63, 6: 59, 7: 93, 8: 75, 9: 95, 10: 6258}
layer  1
{13: 615}

layer  0
{1: 12, 2: 29, 3: 39, 4: 43, 5: 66, 6: 68, 7: 61, 8: 93, 9: 85, 10: 6974}
layer  1
{14: 695}

layer  0
{1: 13, 2: 22, 3: 28, 4: 46, 5: 87, 6: 84, 7: 83, 8: 88, 9: 96, 10: 7226}
layer  1
{15: 673}

layer  0
{1: 12, 2: 20, 3: 46, 4: 56, 5: 69, 6: 83, 7: 107, 8: 87, 9: 106, 10: 7442}
layer  1
{16: 614}

layer  0
{1: 17, 2: 29, 3: 39, 4: 63, 5: 60, 6: 79, 7: 85, 8: 108, 9: 88, 10: 7835}
layer  1
{17: 556}

layer  0
{1: 11, 2: 33, 3: 43, 4: 65, 5: 73, 6: 72, 7: 79, 8: 104, 9: 96, 10: 8253}
layer  1
{18: 604}

layer  0
{1: 15, 2: 31, 3: 33, 4: 63, 5: 87, 6: 94, 7: 99, 8: 120, 9: 109, 10: 8898}
layer  1
{19: 584}

layer  0
{1: 14, 2: 43, 3: 50, 4: 60, 5: 83, 6: 75, 7: 92, 8: 73, 9: 104, 10: 8689}
layer  1
{20: 572}

layer  0
{1: 13, 2: 31, 3: 40, 4: 51, 5: 83, 6: 78, 7: 100, 8: 107, 9: 111, 10: 9765}
layer  1
{21: 586}

layer  0
{1: 17, 2: 31, 3: 40, 4: 61, 5: 76, 6: 99, 7: 105, 8: 104, 9: 125, 10: 10796}
layer  1
{22: 592}

layer  0
{1: 21, 2: 25, 3: 66, 4: 58, 5: 86, 6: 84, 7: 83, 8: 113, 9: 102, 10: 11629}
layer  1
{23: 648}

layer  0
{1: 15, 2: 39, 3: 54, 4: 66, 5: 94, 6: 116, 7: 110, 8: 133, 9: 111, 10: 11759}
layer  1
{24: 645}

layer  0
{1: 2701, 2: 4452, 3: 5925, 4: 7900, 5: 9379, 6: 10469, 7: 11651, 8: 12197, 9: 12752, 10: 1103087}
layer  1
{25: 183378}

data_dict
[[{1: 181, 2: 3, 3: 6, 4: 3, 5: 5, 6: 2, 7: 3, 9: 3, 10: 101}, {1: 173}], [{1: 11, 2: 248, 3: 13, 4: 12, 5: 11, 6: 9, 7: 4, 8: 4, 9: 7, 10: 256}, {2: 221}], [{1: 12, 2: 20, 3: 408, 4: 19, 5: 22, 6: 17, 7: 13, 8: 5, 9: 7, 10: 566}, {3: 374}], [{1: 7, 2: 17, 3: 30, 4: 499, 5: 39, 6: 33, 7: 32, 8: 17, 9: 13, 10: 939}, {4: 456}], [{1: 9, 2: 27, 3: 28, 4: 52, 5: 491, 6: 37, 7: 29, 8: 32, 9: 19, 10: 1302}, {5: 445}], [{1: 9, 2: 26, 3: 35, 4: 32, 5: 61, 6: 616, 7: 44, 8: 28, 9: 34, 10: 1890}, {6: 550}], [{1: 7, 2: 28, 3: 41, 4: 51, 5: 53, 6: 55, 7: 717, 8: 47, 9: 53, 10: 2442}, {7: 655}], [{1: 12, 2: 18, 3: 20, 4: 45, 5: 68, 6: 50, 7: 53, 8: 580, 9: 46, 10: 2785}, {8: 512}], [{1: 9, 2: 16, 3: 28, 4: 37, 5: 43, 6: 50, 7: 54, 8: 56, 9: 628, 10: 3299}, {9: 576}], [{1: 6, 2: 22, 3: 52, 4: 36, 5: 57, 6: 57, 7: 84, 8: 69, 9: 53, 10: 4690}, {10: 640}], [{1: 18, 2: 23, 3: 39, 4: 46, 5: 59, 6: 70, 7: 76, 8: 89, 9: 97, 10: 5246}, {11: 624}], [{1: 21, 2: 35, 3: 38, 4: 49, 5: 59, 6: 61, 7: 74, 8: 83, 9: 72, 10: 5148}, {12: 583}], [{1: 12, 2: 12, 3: 35, 4: 51, 5: 63, 6: 59, 7: 93, 8: 75, 9: 95, 10: 6258}, {13: 615}], [{1: 12, 2: 29, 3: 39, 4: 43, 5: 66, 6: 68, 7: 61, 8: 93, 9: 85, 10: 6974}, {14: 695}], [{1: 13, 2: 22, 3: 28, 4: 46, 5: 87, 6: 84, 7: 83, 8: 88, 9: 96, 10: 7226}, {15: 673}], [{1: 12, 2: 20, 3: 46, 4: 56, 5: 69, 6: 83, 7: 107, 8: 87, 9: 106, 10: 7442}, {16: 614}], [{1: 17, 2: 29, 3: 39, 4: 63, 5: 60, 6: 79, 7: 85, 8: 108, 9: 88, 10: 7835}, {17: 556}], [{1: 11, 2: 33, 3: 43, 4: 65, 5: 73, 6: 72, 7: 79, 8: 104, 9: 96, 10: 8253}, {18: 604}], [{1: 15, 2: 31, 3: 33, 4: 63, 5: 87, 6: 94, 7: 99, 8: 120, 9: 109, 10: 8898}, {19: 584}], [{1: 14, 2: 43, 3: 50, 4: 60, 5: 83, 6: 75, 7: 92, 8: 73, 9: 104, 10: 8689}, {20: 572}], [{1: 13, 2: 31, 3: 40, 4: 51, 5: 83, 6: 78, 7: 100, 8: 107, 9: 111, 10: 9765}, {21: 586}], [{1: 17, 2: 31, 3: 40, 4: 61, 5: 76, 6: 99, 7: 105, 8: 104, 9: 125, 10: 10796}, {22: 592}], [{1: 21, 2: 25, 3: 66, 4: 58, 5: 86, 6: 84, 7: 83, 8: 113, 9: 102, 10: 11629}, {23: 648}], [{1: 15, 2: 39, 3: 54, 4: 66, 5: 94, 6: 116, 7: 110, 8: 133, 9: 111, 10: 11759}, {24: 645}], [{1: 2701, 2: 4452, 3: 5925, 4: 7900, 5: 9379, 6: 10469, 7: 11651, 8: 12197, 9: 12752, 10: 1103087}, {25: 183378}]]
estimated_mem_dict
{0: 0.010282516479492188, 1: 0.026498615741729736, 2: 0.059371769428253174, 3: 0.09871682524681091, 4: 0.13095885515213013, 5: 0.18947795033454895, 6: 0.24908199906349182, 7: 0.2645052373409271, 8: 0.31443214416503906, 9: 0.38681477308273315, 10: 0.43212345242500305, 11: 0.42463982105255127, 12: 0.5091744661331177, 13: 0.5714942514896393, 14: 0.594073086977005, 15: 0.6078604459762573, 16: 0.6297663152217865, 17: 0.6702716946601868, 18: 0.7191262543201447, 19: 0.7044298946857452, 20: 0.7861466109752655, 21: 0.8632670938968658, 22: 0.9334430694580078, 23: 0.9516638517379761, 24: 116.43171608448029}

 MM estimated memory/GB degree 0bucket generated micro-batch: 0.010282516479492188 * 7.971098265895954*0.411
 MM estimated memory/GB degree 1bucket generated micro-batch: 0.026498615741729736 * 7.235294117647059*0.411
 MM estimated memory/GB degree 2bucket generated micro-batch: 0.059371769428253174 * 5.815508021390374*0.411
 MM estimated memory/GB degree 3bucket generated micro-batch: 0.09871682524681091 * 5.720394736842105*0.411
 MM estimated memory/GB degree 4bucket generated micro-batch: 0.13095885515213013 * 6.142022471910112*0.411
 MM estimated memory/GB degree 5bucket generated micro-batch: 0.18947795033454895 * 5.844848484848485*0.411
 MM estimated memory/GB degree 6bucket generated micro-batch: 0.24908199906349182 * 5.279607415485279*0.411
 MM estimated memory/GB degree 7bucket generated micro-batch: 0.2645052373409271 * 6.48583984375*0.411
 MM estimated memory/GB degree 8bucket generated micro-batch: 0.31443214416503906 * 6.019097222222222*0.411
 MM estimated memory/GB degree 9bucket generated micro-batch: 0.38681477308273315 * 5.842499999999999*0.411
 MM estimated memory/GB degree 10bucket generated micro-batch: 0.43212345242500305 * 6.029137529137529*0.411
 MM estimated memory/GB degree 11bucket generated micro-batch: 0.42463982105255127 * 5.878787878787879*0.411
 MM estimated memory/GB degree 12bucket generated micro-batch: 0.5091744661331177 * 6.153721075672295*0.411
 MM estimated memory/GB degree 13bucket generated micro-batch: 0.5714942514896393 * 5.484892086330936*0.411
 MM estimated memory/GB degree 14bucket generated micro-batch: 0.594073086977005 * 5.605151064883605*0.411
 MM estimated memory/GB degree 15bucket generated micro-batch: 0.6078604459762573 * 5.864311889250814*0.411
 MM estimated memory/GB degree 16bucket generated micro-batch: 0.6297663152217865 * 6.3870080406263225*0.411
 MM estimated memory/GB degree 17bucket generated micro-batch: 0.6702716946601868 * 5.798197203826343*0.411
 MM estimated memory/GB degree 18bucket generated micro-batch: 0.7191262543201447 * 6.059571016582552*0.411
 MM estimated memory/GB degree 19bucket generated micro-batch: 0.7044298946857452 * 5.772552447552448*0.411
 MM estimated memory/GB degree 20bucket generated micro-batch: 0.7861466109752655 * 5.934666016577279*0.411
 MM estimated memory/GB degree 21bucket generated micro-batch: 0.8632670938968658 * 6.176597051597051*0.411
 MM estimated memory/GB degree 22bucket generated micro-batch: 0.9334430694580078 * 5.8046833064949*0.411
 MM estimated memory/GB degree 23bucket generated micro-batch: 0.9516638517379761 * 5.688888888888889*0.411
 MM estimated memory/GB degree 24bucket generated micro-batch: 116.43171608448029 * 0.3953612756164862*0.411

modified_estimated_mem_list [:-1]
[0.03368677215355669, 0.07879908950539197, 0.14190884755655406, 0.2320913743152234, 0.33058876703964185, 0.4551701333902099, 0.5404876745903181, 0.7050863675700239, 0.7778576323091984, 0.9288457431234417, 1.0707913386610213, 1.0260070149031553, 1.2877935514528354, 1.288314146222828, 1.3685763217921065, 1.4650848117813295, 1.6531745553242734, 1.5972970284366648, 1.7909722058641298, 1.671273348718595, 1.917527723819148, 2.1914737776176, 2.226938316569812, 2.225116973876953]
sum [:-1] =  27.00486351659401

modified_estimated_mem_list [-1]
18.919395227077814
modified_mem [1, fanout-1]: 
[0.03368677215355669, 0.07879908950539197, 0.14190884755655406, 0.2320913743152234, 0.33058876703964185, 0.4551701333902099, 0.5404876745903181, 0.7050863675700239, 0.7778576323091984, 0.9288457431234417, 1.0707913386610213, 1.0260070149031553, 1.2877935514528354, 1.288314146222828, 1.3685763217921065, 1.4650848117813295, 1.6531745553242734, 1.5972970284366648, 1.7909722058641298, 1.671273348718595, 1.917527723819148, 2.1914737776176, 2.226938316569812, 2.225116973876953]

mem size of fanout degree bucket by formula (GB):  116.43171608448029
the modified memory estimation spend (sec) 0.17272210121154785
the time of number of fanout blocks generation (sec) 11.7288339138031
the time dict collection (sec) 0.1721341609954834
the time estimate mem (sec) 0.0002639293670654297